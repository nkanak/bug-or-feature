{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import contractions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import numpy as np\n",
    "import fasttext\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import networkx as nx\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.nn import norm\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_precision_recall(true_labels, predicted_labels):\n",
    "    return (accuracy_score(true_labels, predicted_labels),\n",
    "           precision_score(true_labels, predicted_labels),\n",
    "           recall_score(true_labels, predicted_labels))\n",
    "\n",
    "def get_random_number():\n",
    "    return random.randint(0, 10000)\n",
    "\n",
    "global_random_number = get_random_number()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('number of node attributes %s' % len(nx.get_node_attributes(G, 'x')[13131]))\n",
    "#print('number of edges %s' % G.size())\n",
    "#print('number of nodes %s' % (len(G)))\n",
    "#x = G.degree()\n",
    "#x = list(dict(x).values())\n",
    "#print('average degree %s' % (sum(x)/len(x)))\n",
    "\n",
    "#nx.draw(g, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407799"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('samples.csv')\n",
    "# bug == 0 and feature == 1\n",
    "df = df[(df['label'] == 0) | (df['label'] == 1)]\n",
    "#df = df[:1000]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions.add('__label__', 'REMOVED_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix contractions\n",
    "df['title'] = df['title'].apply(contractions.fix)\n",
    "df['body'] = df['body'].apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removal of stopwords\n",
    "df['title'] = df['title'].apply(remove_stopwords)\n",
    "df['body'] = df['body'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_of_words_of_title'] = df['title'].str.split().str.len()\n",
    "df['number_of_words_of_body'] = df['body'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Number of words of title')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = sns.histplot(data=df, x='number_of_words_of_title', kde=False, stat='count', binwidth=2)\n",
    "fig.set_xlabel('Number of words of title')\n",
    "#fig.get_figure().savefig('figures/price_kde_histogram.jpg', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Number of words of body')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = sns.histplot(data=df, x='number_of_words_of_body', kde=False, stat='count', binwidth=50)\n",
    "fig.set_xlabel('Number of words of body')\n",
    "#fig.get_figure().savefig('figures/price_kde_histogram.jpg', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    407799.000000\n",
       "mean         45.938685\n",
       "std          63.545595\n",
       "min           0.000000\n",
       "25%          12.000000\n",
       "50%          24.000000\n",
       "75%          52.000000\n",
       "max         977.000000\n",
       "Name: number_of_words_of_body, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['number_of_words_of_body'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    207318\n",
       "0    200481\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bugs == 0\n",
    "# feature == 1\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load glove embeddings 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove2word2vec('glove.6B/glove.6B.100d.txt', 'tmpfile_glove')\n",
    "glove_embeddings_model = KeyedVectors.load_word2vec_format('tmpfile_glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_glove_embedding(word):\n",
    "    if word not in glove_embeddings_model:\n",
    "        return np.zeros(100, dtype='float32')\n",
    "    return glove_embeddings_model.get_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_glove_embedding(sentence):\n",
    "    word_embeddings = [glove_embeddings_model.get_vector(word) if word in glove_embeddings_model else np.zeros(100, dtype='float32') for word in sentence.split()]\n",
    "    if len(word_embeddings) == 0:\n",
    "        return np.zeros(100, dtype='float32')\n",
    "    return np.mean(word_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train fasttext embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fasttext_input'] = '__label__' + df['label'].map(str) + ' ' + df['title'] + ' ' + df['body']\n",
    "train_input, test_input = train_test_split(df.fasttext_input.values, test_size=0.33, random_state=42)\n",
    "np.savetxt('train.txt', train_input, fmt='%s')\n",
    "np.savetxt('test.txt', test_input, fmt='%s')\n",
    "fasttext_model = fasttext.train_supervised('train.txt', dim=100, epoch=5)\n",
    "fasttext_model.test('test.txt')\n",
    "df.drop('fasttext_input', axis=1, inplace=True)\n",
    "embeddings_lookup = {word: fasttext_model.get_word_vector(word) for word in fasttext_model.get_words()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe + logistic regression classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.body, df.label, test_size=0.33, random_state=42)\n",
    "X_train = [get_sentence_glove_embedding(s) for s in X_train.values]\n",
    "X_test = [get_sentence_glove_embedding(s) for s in X_test.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_regression_classifier(use_global_random_number=True):\n",
    "    if use_global_random_number:\n",
    "        random_state = global_random_number\n",
    "    else:\n",
    "        random_state = get_random_number()\n",
    "    logit_clf = LogisticRegression(random_state=random_state, solver='liblinear').fit(X_train, y_train)\n",
    "    predicted_labels = logit_clf.predict(X_test)\n",
    "    true_labels = y_test\n",
    "    \n",
    "    results = calculate_accuracy_precision_recall(true_labels, predicted_labels)\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.722695320046963, 0.7408314087759815, 0.7013935453776858)\n"
     ]
    }
   ],
   "source": [
    "logit_evaluation_results = []\n",
    "logit_evaluation_results.append(run_logistic_regression_classifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe + knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_knn_regression_classifier():\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=10).fit(X_train, y_train)\n",
    "    predicted_labels = knn_clf.predict(X_test)\n",
    "    true_labels = y_test\n",
    "    \n",
    "    results = calculate_accuracy_precision_recall(true_labels, predicted_labels)\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6918052521289402, 0.7341481519843942, 0.6199090405527535)\n"
     ]
    }
   ],
   "source": [
    "knn_evaluation_results = []\n",
    "knn_evaluation_results.append(run_knn_regression_classifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7966471978242454, 0.813314895298301, 0.7801667589866185)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, test_df = train_test_split(df, test_size=0.33, random_state=42)\n",
    "predicted_labels = [int(res[0].split('__label__')[1]) for res in fasttext_model.predict(test_df['body'].values.tolist())[0]]\n",
    "true_labels = test_df['label'].values\n",
    "calculate_accuracy_precision_recall(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.body, df.label, test_size=0.33, random_state=42)\n",
    "x_train = [get_sentence_glove_embedding(s) for s in x_train.values]\n",
    "x_test = [get_sentence_glove_embedding(s) for s in x_test.values]\n",
    "\n",
    "x_train = torch.tensor(x_train).float()\n",
    "y_train = torch.tensor(y_train.values).float()\n",
    "\n",
    "x_test = torch.tensor(x_test).float()\n",
    "y_test = torch.tensor(y_test.values).float()\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "BATCH_SIZE = 255\n",
    "\n",
    "# generate train dataset and train dataloader\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# generate test dataset\n",
    "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self) : \n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(100 , 250)\n",
    "        self.linear2 = torch.nn.Linear(250, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p=0.5, training=self.training)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mlp_classifier():\n",
    "    # Initialize the MLP\n",
    "    mlp = MLP().to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = F.binary_cross_entropy\n",
    "    optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "    mlp.train()\n",
    "    # Run the training loop\n",
    "    for epoch in range(0, 50):\n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, (inputs, targets) in enumerate(train_dataloader):\n",
    "            # Get inputs\n",
    "            targets = targets.squeeze()\n",
    "\n",
    "            # Perform forward pass\n",
    "            outputs = mlp(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "\n",
    "\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(f'Epoch: {epoch}, Epoch loss {loss.item()}')\n",
    "    # Process is complete.\n",
    "    print('Training process has finished.')\n",
    "    print('Final loss', loss.item())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mlp.eval()\n",
    "        pred = mlp(test_dataset.tensors[0].to(device))\n",
    "        y_true = test_dataset.tensors[1].tolist()\n",
    "        y_pred = torch.round(torch.tensor(pred.tolist()).squeeze())\n",
    "        results = calculate_accuracy_precision_recall(y_true, y_pred)\n",
    "\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Epoch loss 0.5467864274978638\n",
      "Epoch: 1, Epoch loss 0.5287327170372009\n",
      "Epoch: 2, Epoch loss 0.5218595862388611\n",
      "Epoch: 3, Epoch loss 0.5106785297393799\n",
      "Epoch: 4, Epoch loss 0.5000659823417664\n",
      "Epoch: 5, Epoch loss 0.5005460381507874\n",
      "Epoch: 6, Epoch loss 0.48574137687683105\n",
      "Epoch: 7, Epoch loss 0.4874698519706726\n",
      "Epoch: 8, Epoch loss 0.47833412885665894\n",
      "Epoch: 9, Epoch loss 0.4708586633205414\n",
      "Training process has finished.\n",
      "Final loss 0.4708586633205414\n",
      "(0.7306686284126206, 0.7485596644594325, 0.7102271070814262)\n"
     ]
    }
   ],
   "source": [
    "mlp_evaluation_results = []\n",
    "mlp_evaluation_results.append(run_mlp_classifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_of_words(text, window_size):\n",
    "    text = text.split()\n",
    "    G = nx.Graph()\n",
    "    for i, word in enumerate(text):\n",
    "        #embedding = fasttext_model.get_word_vector(word)\n",
    "        embedding = embeddings_lookup.get(word, np.zeros(100, dtype='float32'))\n",
    "        G.add_node(word, x=embedding)\n",
    "        for j in range(i + 1, i + window_size):\n",
    "            if j < len(text):\n",
    "                G.add_edge(word, text[j])\n",
    "    return G\n",
    "\n",
    "def create_graph_of_words_for_pytorch(text, window_size):\n",
    "    return from_networkx(create_graph_of_words(text, window_size))\n",
    "\n",
    "def generate_pytorch_geometric_graphs(window_size):\n",
    "    pyg_graphs = []\n",
    "    for s in tqdm(df['body'].values):\n",
    "        pyg_graphs.append(create_graph_of_words_for_pytorch(s, window_size))\n",
    "    print('finished...')\n",
    "    for i, label in enumerate(df['label'].values):\n",
    "        pyg_graphs[i].y = torch.tensor(label).float()\n",
    "    \n",
    "    pyg_graphs = [g for g in pyg_graphs if g.num_nodes != 0]\n",
    "    return pyg_graphs\n",
    "\n",
    "class GATClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        \n",
    "        self.conv1 = GATConv(100, 10, heads=3)\n",
    "        self.linear1 = torch.nn.Linear(10*3, 1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, data, batch):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 407799/407799 [18:09<00:00, 374.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished...\n"
     ]
    }
   ],
   "source": [
    "pytorch_geometric_graphs = generate_pytorch_geometric_graphs(window_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pyg_graphs, test_pyg_graphs = train_test_split(pytorch_geometric_graphs, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gat_classifier(train_batch_size=300, learning_rate=0.001, num_epoch=10):\n",
    "    train_loader = DataLoader(train_pyg_graphs, batch_size=train_batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_pyg_graphs, batch_size=200, shuffle=False)\n",
    "    \n",
    "    gat_model = GATClassifier().to(device)\n",
    "    print(gat_model)\n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = F.binary_cross_entropy\n",
    "    optimizer = torch.optim.Adam(gat_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    gat_model.train()\n",
    "    for epoch in range(0, num_epoch):\n",
    "        for i, data in enumerate(train_loader):  # Iterate in batches over the training dataset.\n",
    "            data = data.to(device)\n",
    "            try:\n",
    "                out = gat_model(data, data.batch)  # Perform a single forward pass.\n",
    "            except Exception as e:\n",
    "                print(data)\n",
    "                print(data.x)\n",
    "                print(data.y)\n",
    "            out = out.squeeze()\n",
    "            y = data.y.squeeze()\n",
    "            loss = loss_function(out, y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "        print(f'Epoch: {epoch}, Epoch loss {loss.item()}')\n",
    "\n",
    "    print('Training process has finished.')\n",
    "    print('Final loss', loss.item())\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    with torch.no_grad():\n",
    "        gat_model.eval()\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            out = gat_model(data, data.batch)\n",
    "            pred_labels.extend(torch.round(out.squeeze()).tolist())\n",
    "            true_labels.extend(data.y.tolist())\n",
    "    \n",
    "    results = calculate_accuracy_precision_recall(true_labels, pred_labels)\n",
    "    \n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GATClassifier(\n",
      "  (conv1): GATConv(100, 10, heads=3)\n",
      "  (linear1): Linear(in_features=30, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch: 0, Epoch loss 0.34798863530158997\n",
      "Epoch: 1, Epoch loss 0.3490433990955353\n",
      "Epoch: 2, Epoch loss 0.34401196241378784\n",
      "Epoch: 3, Epoch loss 0.338461697101593\n",
      "Epoch: 4, Epoch loss 0.330524742603302\n",
      "Epoch: 5, Epoch loss 0.3304673731327057\n",
      "Epoch: 6, Epoch loss 0.32263198494911194\n",
      "Epoch: 7, Epoch loss 0.31862515211105347\n",
      "Epoch: 8, Epoch loss 0.32849597930908203\n",
      "Epoch: 9, Epoch loss 0.3328363001346588\n",
      "Training process has finished.\n",
      "Final loss 0.3328363001346588\n",
      "(0.8025250048300563, 0.8109618356913566, 0.797509609341888)\n"
     ]
    }
   ],
   "source": [
    "gat_evaluation_results = []\n",
    "gat_evaluation_results.append(run_gat_classifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove + GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_of_words_glove(text, window_size):\n",
    "    text = text.split()\n",
    "    G = nx.Graph()\n",
    "    for i, word in enumerate(text):\n",
    "        #embedding = fasttext_model.get_word_vector(word)\n",
    "        embedding = get_word_glove_embedding(word)\n",
    "        G.add_node(word, x=embedding)\n",
    "        for j in range(i + 1, i + window_size):\n",
    "            if j < len(text):\n",
    "                G.add_edge(word, text[j])\n",
    "    return G\n",
    "\n",
    "def create_graph_of_words_for_pytorch_glove(text, window_size):\n",
    "    return from_networkx(create_graph_of_words_glove(text, window_size))\n",
    "\n",
    "def generate_pytorch_geometric_graphs_glove(window_size):\n",
    "    pyg_graphs = []\n",
    "    for s in tqdm(df['body'].values):\n",
    "        pyg_graphs.append(create_graph_of_words_for_pytorch_glove(s, window_size))\n",
    "    print('finished...')\n",
    "    for i, label in enumerate(df['label'].values):\n",
    "        pyg_graphs[i].y = torch.tensor(label).float()\n",
    "    \n",
    "    pyg_graphs = [g for g in pyg_graphs if g.num_nodes != 0]\n",
    "    return pyg_graphs\n",
    "\n",
    "class GATGloveClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        \n",
    "        self.conv1 = GATConv(100, 10, heads=3)\n",
    "        self.linear1 = torch.nn.Linear(10*3, 1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, data, batch):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 407799/407799 [17:37<00:00, 385.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished...\n"
     ]
    }
   ],
   "source": [
    "pytorch_geometric_graphs_glove = generate_pytorch_geometric_graphs_glove(window_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pyg_graphs_glove, test_pyg_graphs_glove = train_test_split(pytorch_geometric_graphs_glove, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gat_glove_classifier(train_batch_size=300, learning_rate=0.001, num_epoch=10):\n",
    "    train_loader = DataLoader(train_pyg_graphs_glove, batch_size=train_batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_pyg_graphs_glove, batch_size=200, shuffle=False)\n",
    "    \n",
    "    gat_model = GATGloveClassifier().to(device)\n",
    "    print(gat_model)\n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = F.binary_cross_entropy\n",
    "    optimizer = torch.optim.Adam(gat_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    gat_model.train()\n",
    "    for epoch in range(0, num_epoch):\n",
    "        for i, data in enumerate(train_loader):  # Iterate in batches over the training dataset.\n",
    "            data = data.to(device)\n",
    "            try:\n",
    "                out = gat_model(data, data.batch)  # Perform a single forward pass.\n",
    "            except Exception as e:\n",
    "                print(data)\n",
    "                print(data.x)\n",
    "                print(data.y)\n",
    "            out = out.squeeze()\n",
    "            y = data.y.squeeze()\n",
    "            loss = loss_function(out, y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "        print(f'Epoch: {epoch}, Epoch loss {loss.item()}')\n",
    "\n",
    "    print('Training process has finished.')\n",
    "    print('Final loss', loss.item())\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    with torch.no_grad():\n",
    "        gat_model.eval()\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            out = gat_model(data, data.batch)\n",
    "            pred_labels.extend(torch.round(out.squeeze()).tolist())\n",
    "            true_labels.extend(data.y.tolist())\n",
    "    \n",
    "    results = calculate_accuracy_precision_recall(true_labels, pred_labels)\n",
    "    \n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GATGloveClassifier(\n",
      "  (conv1): GATConv(100, 10, heads=3)\n",
      "  (linear1): Linear(in_features=30, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch: 0, Epoch loss 0.5432955622673035\n",
      "Epoch: 1, Epoch loss 0.5186551809310913\n",
      "Epoch: 2, Epoch loss 0.49953871965408325\n",
      "Epoch: 3, Epoch loss 0.5260472297668457\n",
      "Epoch: 4, Epoch loss 0.4987495541572571\n",
      "Epoch: 5, Epoch loss 0.5135094523429871\n",
      "Epoch: 6, Epoch loss 0.4957980215549469\n",
      "Epoch: 7, Epoch loss 0.5137096643447876\n",
      "Epoch: 8, Epoch loss 0.5037187933921814\n",
      "Epoch: 9, Epoch loss 0.5215823650360107\n",
      "Training process has finished.\n",
      "Final loss 0.5215823650360107\n",
      "(0.7637359371052358, 0.7733188072709907, 0.7573038305832834)\n"
     ]
    }
   ],
   "source": [
    "gat_glove_evaluation_results = []\n",
    "gat_glove_evaluation_results.append(run_gat_glove_classifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove + GCNonv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNGloveClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        \n",
    "        self.conv1 = GCNConv(100, 10)\n",
    "        self.linear1 = torch.nn.Linear(10, 1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, data, batch):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gcn_glove_classifier(train_batch_size=300, learning_rate=0.001, num_epoch=10):\n",
    "    train_loader = DataLoader(train_pyg_graphs_glove, batch_size=train_batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_pyg_graphs_glove, batch_size=200, shuffle=False)\n",
    "    \n",
    "    model = GCNGloveClassifier().to(device)\n",
    "    print(model)\n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = F.binary_cross_entropy\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(0, num_epoch):\n",
    "        for i, data in enumerate(train_loader):  # Iterate in batches over the training dataset.\n",
    "            data = data.to(device)\n",
    "            try:\n",
    "                out = model(data, data.batch)  # Perform a single forward pass.\n",
    "            except Exception as e:\n",
    "                print(data)\n",
    "                print(data.x)\n",
    "                print(data.y)\n",
    "            out = out.squeeze()\n",
    "            y = data.y.squeeze()\n",
    "            loss = loss_function(out, y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "        print(f'Epoch: {epoch}, Epoch loss {loss.item()}')\n",
    "\n",
    "    print('Training process has finished.')\n",
    "    print('Final loss', loss.item())\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            out = model(data, data.batch)\n",
    "            pred_labels.extend(torch.round(out.squeeze()).tolist())\n",
    "            true_labels.extend(data.y.tolist())\n",
    "    \n",
    "    results = calculate_accuracy_precision_recall(true_labels, pred_labels)\n",
    "    \n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNGloveClassifier(\n",
      "  (conv1): GCNConv(100, 10)\n",
      "  (linear1): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch: 0, Epoch loss 0.5705499053001404\n",
      "Epoch: 1, Epoch loss 0.5697073340415955\n",
      "Epoch: 2, Epoch loss 0.5562869310379028\n",
      "Epoch: 3, Epoch loss 0.5690243244171143\n",
      "Epoch: 4, Epoch loss 0.5552098155021667\n",
      "Epoch: 5, Epoch loss 0.5686145424842834\n",
      "Epoch: 6, Epoch loss 0.5702721476554871\n",
      "Epoch: 7, Epoch loss 0.5903779864311218\n",
      "Epoch: 8, Epoch loss 0.5587636828422546\n",
      "Epoch: 9, Epoch loss 0.5603534579277039\n",
      "Training process has finished.\n",
      "Final loss 0.5603534579277039\n",
      "(0.7294053829120039, 0.7465643680285943, 0.7082121508849364)\n"
     ]
    }
   ],
   "source": [
    "gcn_glove_evaluation_results = []\n",
    "gcn_glove_evaluation_results.append(run_gcn_glove_classifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove + GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvGloveClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        \n",
    "        self.conv1 = GraphConv(100, 10, aggr='mean')\n",
    "        self.linear1 = torch.nn.Linear(10, 1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, data, batch):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_graphconv_glove_classifier(train_batch_size=300, learning_rate=0.001, num_epoch=10):\n",
    "    train_loader = DataLoader(train_pyg_graphs_glove, batch_size=train_batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_pyg_graphs_glove, batch_size=200, shuffle=False)\n",
    "    \n",
    "    model = GraphConvGloveClassifier().to(device)\n",
    "    print(model)\n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = F.binary_cross_entropy\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(0, num_epoch):\n",
    "        for i, data in enumerate(train_loader):  # Iterate in batches over the training dataset.\n",
    "            data = data.to(device)\n",
    "            try:\n",
    "                out = model(data, data.batch)  # Perform a single forward pass.\n",
    "            except Exception as e:\n",
    "                print(data)\n",
    "                print(data.x)\n",
    "                print(data.y)\n",
    "            out = out.squeeze()\n",
    "            y = data.y.squeeze()\n",
    "            loss = loss_function(out, y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "        print(f'Epoch: {epoch}, Epoch loss {loss.item()}')\n",
    "\n",
    "    print('Training process has finished.')\n",
    "    print('Final loss', loss.item())\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            out = model(data, data.batch)\n",
    "            pred_labels.extend(torch.round(out.squeeze()).tolist())\n",
    "            true_labels.extend(data.y.tolist())\n",
    "    \n",
    "    results = calculate_accuracy_precision_recall(true_labels, pred_labels)\n",
    "    \n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphConvGloveClassifier(\n",
      "  (conv1): GraphConv(100, 10)\n",
      "  (linear1): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch: 0, Epoch loss 0.556878924369812\n",
      "Epoch: 1, Epoch loss 0.5519242286682129\n",
      "Epoch: 2, Epoch loss 0.5434998273849487\n",
      "Epoch: 3, Epoch loss 0.5496436953544617\n",
      "Epoch: 4, Epoch loss 0.546381950378418\n",
      "Epoch: 5, Epoch loss 0.5566105246543884\n",
      "Epoch: 6, Epoch loss 0.5527468919754028\n",
      "Epoch: 7, Epoch loss 0.5617696046829224\n",
      "Epoch: 8, Epoch loss 0.531860888004303\n",
      "Epoch: 9, Epoch loss 0.5340650081634521\n",
      "Training process has finished.\n",
      "Final loss 0.5340650081634521\n",
      "(0.7483986505565711, 0.7600595900923948, 0.7381874515879163)\n"
     ]
    }
   ],
   "source": [
    "graphconv_glove_evaluation_results = []\n",
    "graphconv_glove_evaluation_results.append(run_graphconv_glove_classifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove + SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEConvGloveClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        \n",
    "        self.conv1 = SAGEConv(100, 10)\n",
    "        self.linear1 = torch.nn.Linear(10, 1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, data, batch):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sageconv_glove_classifier(train_batch_size=300, learning_rate=0.001, num_epoch=10):\n",
    "    train_loader = DataLoader(train_pyg_graphs_glove, batch_size=train_batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_pyg_graphs_glove, batch_size=200, shuffle=False)\n",
    "    \n",
    "    model = SAGEConvGloveClassifier().to(device)\n",
    "    print(model)\n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = F.binary_cross_entropy\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(0, num_epoch):\n",
    "        for i, data in enumerate(train_loader):  # Iterate in batches over the training dataset.\n",
    "            data = data.to(device)\n",
    "            try:\n",
    "                out = model(data, data.batch)  # Perform a single forward pass.\n",
    "            except Exception as e:\n",
    "                print(data)\n",
    "                print(data.x)\n",
    "                print(data.y)\n",
    "            out = out.squeeze()\n",
    "            y = data.y.squeeze()\n",
    "            loss = loss_function(out, y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "        print(f'Epoch: {epoch}, Epoch loss {loss.item()}')\n",
    "\n",
    "    print('Training process has finished.')\n",
    "    print('Final loss', loss.item())\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            out = model(data, data.batch)\n",
    "            pred_labels.extend(torch.round(out.squeeze()).tolist())\n",
    "            true_labels.extend(data.y.tolist())\n",
    "    \n",
    "    results = calculate_accuracy_precision_recall(true_labels, pred_labels)\n",
    "    \n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAGEConvGloveClassifier(\n",
      "  (conv1): SAGEConv(100, 10)\n",
      "  (linear1): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch: 0, Epoch loss 0.5568789839744568\n",
      "Epoch: 1, Epoch loss 0.5519242286682129\n",
      "Epoch: 2, Epoch loss 0.5434998273849487\n",
      "Epoch: 3, Epoch loss 0.5496436953544617\n",
      "Epoch: 4, Epoch loss 0.546381950378418\n",
      "Epoch: 5, Epoch loss 0.5566105246543884\n",
      "Epoch: 6, Epoch loss 0.5527468919754028\n",
      "Epoch: 7, Epoch loss 0.5617696046829224\n",
      "Epoch: 8, Epoch loss 0.5318608283996582\n",
      "Epoch: 9, Epoch loss 0.5340650081634521\n",
      "Epoch: 10, Epoch loss 0.5269910097122192\n",
      "Epoch: 11, Epoch loss 0.5415917634963989\n",
      "Epoch: 12, Epoch loss 0.5225735902786255\n",
      "Epoch: 13, Epoch loss 0.5119897723197937\n",
      "Epoch: 14, Epoch loss 0.5248987674713135\n",
      "Epoch: 15, Epoch loss 0.5316376686096191\n",
      "Epoch: 16, Epoch loss 0.5322040915489197\n",
      "Epoch: 17, Epoch loss 0.5357478857040405\n",
      "Epoch: 18, Epoch loss 0.541910707950592\n",
      "Epoch: 19, Epoch loss 0.5312440395355225\n",
      "Epoch: 20, Epoch loss 0.5123745203018188\n",
      "Epoch: 21, Epoch loss 0.5106387138366699\n",
      "Epoch: 22, Epoch loss 0.5031642913818359\n",
      "Epoch: 23, Epoch loss 0.4981933832168579\n",
      "Epoch: 24, Epoch loss 0.5303360223770142\n",
      "Epoch: 25, Epoch loss 0.5085858702659607\n",
      "Epoch: 26, Epoch loss 0.5223764181137085\n",
      "Epoch: 27, Epoch loss 0.5063081383705139\n",
      "Epoch: 28, Epoch loss 0.5025570392608643\n",
      "Epoch: 29, Epoch loss 0.49964189529418945\n",
      "Epoch: 30, Epoch loss 0.5167818069458008\n",
      "Epoch: 31, Epoch loss 0.5059103965759277\n",
      "Epoch: 32, Epoch loss 0.5160890221595764\n",
      "Epoch: 33, Epoch loss 0.5020461082458496\n",
      "Epoch: 34, Epoch loss 0.5131187438964844\n",
      "Epoch: 35, Epoch loss 0.5020177364349365\n",
      "Epoch: 36, Epoch loss 0.517825722694397\n",
      "Epoch: 37, Epoch loss 0.52492755651474\n",
      "Epoch: 38, Epoch loss 0.5149352550506592\n",
      "Epoch: 39, Epoch loss 0.518298327922821\n",
      "Epoch: 40, Epoch loss 0.5048672556877136\n",
      "Epoch: 41, Epoch loss 0.5010474920272827\n",
      "Epoch: 42, Epoch loss 0.4985220432281494\n",
      "Epoch: 43, Epoch loss 0.5170873403549194\n",
      "Epoch: 44, Epoch loss 0.5056470036506653\n",
      "Epoch: 45, Epoch loss 0.49902039766311646\n",
      "Epoch: 46, Epoch loss 0.5000661015510559\n",
      "Epoch: 47, Epoch loss 0.5190498232841492\n",
      "Epoch: 48, Epoch loss 0.5159509181976318\n",
      "Epoch: 49, Epoch loss 0.5124653577804565\n",
      "Training process has finished.\n",
      "Final loss 0.5124653577804565\n",
      "(0.7617444677277929, 0.7752877044215627, 0.7482863949256828)\n"
     ]
    }
   ],
   "source": [
    "sageconv_glove_evaluation_results = []\n",
    "sageconv_glove_evaluation_results.append(run_sageconv_glove_classifier(num_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
