{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import contractions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import fasttext\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import networkx as nx\n",
    "from torch_geometric.utils.convert import from_networkx, to_networkx\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from torch_geometric.nn import GATConv\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn.models import GNNExplainer\n",
    "from torch_geometric.data import Data\n",
    "import random\n",
    "from inspect import signature\n",
    "from math import sqrt\n",
    "import torch\n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# Set fixed random number seed\n",
    "#torch.manual_seed(42)\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def calculate_accuracy_precision_recall(true_labels, predicted_labels):\n",
    "    return (accuracy_score(true_labels, predicted_labels),\n",
    "           precision_score(true_labels, predicted_labels),\n",
    "           recall_score(true_labels, predicted_labels))\n",
    "\n",
    "def print_evaluation_results(results):\n",
    "    print('Avg accuracy | Avg precision | Avg recall')\n",
    "    avg_accuracy, avg_precision, avg_recall = np.mean(results, axis=0)\n",
    "    std_accuracy, std_precision, std_recall = np.std(results, axis=0)\n",
    "    print(f'{avg_accuracy:.4f}+-{std_accuracy:.4f}, {avg_precision:.4f}+-{std_precision:.4f}, {avg_recall:.4f}+-{std_recall:.4f}')\n",
    "\n",
    "def get_random_number():\n",
    "    return random.randint(0, 10000)\n",
    "\n",
    "global_random_number = [get_random_number()]\n",
    "global_random_numbers = [get_random_number() for _ in range(10)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "407799"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('samples.csv')\n",
    "# bug == 0 and feature == 1\n",
    "df = df[(df['label'] == 0) | (df['label'] == 1)]\n",
    "#df = df[:500]\n",
    "len(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "contractions.add('__label__', 'REMOVED_TOKEN')\n",
    "# fix contractions\n",
    "df['title'] = df['title'].apply(contractions.fix)\n",
    "df['body'] = df['body'].apply(contractions.fix)\n",
    "# removal of stopwords\n",
    "df['title'] = df['title'].apply(remove_stopwords)\n",
    "df['body'] = df['body'].apply(remove_stopwords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22700\\3057863787.py:1: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec('glove.6B/glove.6B.100d.txt', 'tmpfile_glove')\n"
     ]
    }
   ],
   "source": [
    "glove2word2vec('glove.6B/glove.6B.100d.txt', 'tmpfile_glove')\n",
    "glove_embeddings_model = KeyedVectors.load_word2vec_format('tmpfile_glove')\n",
    "\n",
    "def get_word_glove_embedding(word):\n",
    "    if word not in glove_embeddings_model:\n",
    "        return np.zeros(100, dtype='float32')\n",
    "    return glove_embeddings_model.get_vector(word)\n",
    "\n",
    "\n",
    "def get_sentence_glove_embedding(sentence):\n",
    "    word_embeddings = [\n",
    "        glove_embeddings_model.get_vector(word) if word in glove_embeddings_model else np.zeros(100, dtype='float32')\n",
    "        for word in sentence.split()]\n",
    "    if len(word_embeddings) == 0:\n",
    "        return np.zeros(100, dtype='float32')\n",
    "    return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "df['fasttext_input'] = '__label__' + df['label'].map(str) + ' ' + df['title'] + ' ' + df['body']\n",
    "train_input, test_input = train_test_split(df.fasttext_input.values, test_size=0.33, random_state=42)\n",
    "np.savetxt('train.txt', train_input, fmt='%s')\n",
    "np.savetxt('test.txt', test_input, fmt='%s')\n",
    "fasttext_model = fasttext.train_supervised('train.txt', dim=100, epoch=5)\n",
    "fasttext_model.test('test.txt')\n",
    "df.drop('fasttext_input', axis=1, inplace=True)\n",
    "embeddings_lookup = {word: fasttext_model.get_word_vector(word) for word in fasttext_model.get_words()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/273225 [00:00<?, ?it/s]D:\\WINDOWS DATA\\Development\\ictai journal extension\\env\\lib\\site-packages\\torch_geometric\\utils\\convert.py:192: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  data[key] = torch.tensor(value)\n",
      "100%|██████████| 273225/273225 [12:22<00:00, 368.16it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134574/134574 [06:10<00:00, 362.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished...\n"
     ]
    }
   ],
   "source": [
    "def create_graph_of_words(text, window_size):\n",
    "    text = text.split()\n",
    "    embeddings_for_text = []\n",
    "    G = nx.Graph()\n",
    "    for i, word in enumerate(text):\n",
    "        #embedding = fasttext_model.get_word_vector(word)\n",
    "        embedding = embeddings_lookup.get(word, np.zeros(100, dtype='float32'))\n",
    "        G.add_node(word, x=embedding)\n",
    "        embeddings_for_text.append(embedding)\n",
    "        for j in range(i + 1, i + window_size):\n",
    "            if j < len(text):\n",
    "                G.add_edge(word, text[j])\n",
    "    return G\n",
    "\n",
    "\n",
    "def create_graph_of_words_for_pytorch(text, window_size):\n",
    "    G  = create_graph_of_words(text, window_size)\n",
    "    return G, from_networkx(G)\n",
    "\n",
    "\n",
    "def generate_pytorch_geometric_graphs(data, window_size):\n",
    "    netx_graphs = []\n",
    "    pyg_graphs = []\n",
    "\n",
    "    for s in tqdm(data['body'].values):\n",
    "        netx_graph, pyg_graph = create_graph_of_words_for_pytorch(s, window_size)\n",
    "        pyg_graphs.append(pyg_graph)\n",
    "        netx_graphs.append(netx_graph)\n",
    "    print('finished...')\n",
    "\n",
    "    for i, label in enumerate(data['label'].values):\n",
    "        pyg_graphs[i].y = torch.tensor(label).float()\n",
    "        netx_graphs[i].graph['y']=label\n",
    "\n",
    "    # print(len(pyg_graphs),len(netx_graphs))\n",
    "    return pyg_graphs, netx_graphs\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.33, random_state=42)\n",
    "train_pyg_graphs, train_netx_graphs = generate_pytorch_geometric_graphs(train_df, window_size=7)\n",
    "test_pyg_graphs, test_netx_graphs = generate_pytorch_geometric_graphs(test_df, window_size=7)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    title  \\\n",
      "0       include \\ external assigned\\ slots mission lis...   \n",
      "1       tke, like mark task imcomplete, progress, comp...   \n",
      "2       thread monitors checkboxes oscilloscope activi...   \n",
      "3                        crash camera plugin usage ios 10   \n",
      "4       identicon display contact list friends running...   \n",
      "...                                                   ...   \n",
      "273220                         allow directing csv stdout   \n",
      "273221                                   generate pledges   \n",
      "273222  systemimager: fix upstream dhcp option- flags ...   \n",
      "273223                        add flags temperature units   \n",
      "273224                              epanet mtp2 run error   \n",
      "\n",
      "                                                     body  label  \n",
      "0       blocked slots currently counted \\ players\\ val...      0  \n",
      "1       problem \\are \\r exists, users change status as...      1  \n",
      "2       actual behaviour \\are \\r thread checks checkbo...      0  \n",
      "3       app crash use camera plugin .\\are \\r error is:...      0  \n",
      "4       brief description\\are \\r os: ubuntu 16.04\\r co...      0  \n",
      "...                                                   ...    ...  \n",
      "273220  general reporters taking io.writer argument, a...      1  \n",
      "273221  thanks creating issue! form sure information n...      1  \n",
      "273222  reported olahaye74 20 feb 2014 15:41 utc need ...      0  \n",
      "273223  program shouild eventually set use flags deter...      1  \n",
      "273224  hi,\\are \\r installed epanet mtp2 mtp1 windows ...      0  \n",
      "\n",
      "[273225 rows x 3 columns]\n",
      "                                                    title  \\\n",
      "0                               api autocomplete medicine   \n",
      "1       gettau function return correct crystal type gi...   \n",
      "2                 gobblin yarn overwrites job start date.   \n",
      "3                font mention italic notifications screen   \n",
      "4                                  add support url-loader   \n",
      "...                                                   ...   \n",
      "134569                            multiple providers blas   \n",
      "134570                        naming primitive operations   \n",
      "134571                     add \\ about\\ snippet home page   \n",
      "134572  kurento-client crashes node.js >=6.3 dependenc...   \n",
      "134573                           integrate json web token   \n",
      "\n",
      "                                                     body  label  \n",
      "0       add autocomplete text box search names common ...      1  \n",
      "1       gettau function resolution library return prop...      0  \n",
      "2       gobblinhelixjoblauncher.waitforjobcompletion m...      0  \n",
      "3                                           posts italic.      0  \n",
      "4       add support https://github.com/webpack-contrib...      1  \n",
      "...                                                   ...    ...  \n",
      "134569  fresh spack ae9a9e019a73cb951d4d2a2585ac71a53f...      0  \n",
      "134570  stated protocol https://github.com/aeternity/p...      1  \n",
      "134571  add small snippet \\ me\\ home page link \\ read ...      1  \n",
      "134572  node 6.4 kurento-client@6.5.0 connected latest...      1  \n",
      "134573  integrate json web token filter generation met...      1  \n",
      "\n",
      "[134574 rows x 3 columns]\n",
      "273225 273225 273225\n",
      "134574 134574 134574\n"
     ]
    }
   ],
   "source": [
    "print(train_df)\n",
    "print(test_df)\n",
    "print(len(train_pyg_graphs),len(train_netx_graphs), len(train_df))\n",
    "print(len(test_pyg_graphs),len(test_netx_graphs), len(test_df))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Error in train graph 36901\n",
      "                                                    title  \\\n",
      "0       include \\ external assigned\\ slots mission lis...   \n",
      "1       tke, like mark task imcomplete, progress, comp...   \n",
      "2       thread monitors checkboxes oscilloscope activi...   \n",
      "3                        crash camera plugin usage ios 10   \n",
      "4       identicon display contact list friends running...   \n",
      "...                                                   ...   \n",
      "273219                         allow directing csv stdout   \n",
      "273220                                   generate pledges   \n",
      "273221  systemimager: fix upstream dhcp option- flags ...   \n",
      "273222                        add flags temperature units   \n",
      "273223                              epanet mtp2 run error   \n",
      "\n",
      "                                                     body  label  \n",
      "0       blocked slots currently counted \\ players\\ val...      0  \n",
      "1       problem \\are \\r exists, users change status as...      1  \n",
      "2       actual behaviour \\are \\r thread checks checkbo...      0  \n",
      "3       app crash use camera plugin .\\are \\r error is:...      0  \n",
      "4       brief description\\are \\r os: ubuntu 16.04\\r co...      0  \n",
      "...                                                   ...    ...  \n",
      "273219  general reporters taking io.writer argument, a...      1  \n",
      "273220  thanks creating issue! form sure information n...      1  \n",
      "273221  reported olahaye74 20 feb 2014 15:41 utc need ...      0  \n",
      "273222  program shouild eventually set use flags deter...      1  \n",
      "273223  hi,\\are \\r installed epanet mtp2 mtp1 windows ...      0  \n",
      "\n",
      "[273224 rows x 3 columns]\n",
      "                                                    title  \\\n",
      "0                               api autocomplete medicine   \n",
      "1       gettau function return correct crystal type gi...   \n",
      "2                 gobblin yarn overwrites job start date.   \n",
      "3                font mention italic notifications screen   \n",
      "4                                  add support url-loader   \n",
      "...                                                   ...   \n",
      "134569                            multiple providers blas   \n",
      "134570                        naming primitive operations   \n",
      "134571                     add \\ about\\ snippet home page   \n",
      "134572  kurento-client crashes node.js >=6.3 dependenc...   \n",
      "134573                           integrate json web token   \n",
      "\n",
      "                                                     body  label  \n",
      "0       add autocomplete text box search names common ...      1  \n",
      "1       gettau function resolution library return prop...      0  \n",
      "2       gobblinhelixjoblauncher.waitforjobcompletion m...      0  \n",
      "3                                           posts italic.      0  \n",
      "4       add support https://github.com/webpack-contrib...      1  \n",
      "...                                                   ...    ...  \n",
      "134569  fresh spack ae9a9e019a73cb951d4d2a2585ac71a53f...      0  \n",
      "134570  stated protocol https://github.com/aeternity/p...      1  \n",
      "134571  add small snippet \\ me\\ home page link \\ read ...      1  \n",
      "134572  node 6.4 kurento-client@6.5.0 connected latest...      1  \n",
      "134573  integrate json web token filter generation met...      1  \n",
      "\n",
      "[134574 rows x 3 columns]\n",
      "273224 273224 273224\n",
      "134574 134574 134574\n"
     ]
    }
   ],
   "source": [
    "id_error_train = []\n",
    "for i, data in enumerate(train_pyg_graphs):\n",
    "    if data.x == None:\n",
    "        print(f'Found Error in train graph {i}')\n",
    "        id_error_train.append(i)\n",
    "\n",
    "id_error_test = []\n",
    "for i, data in enumerate(test_pyg_graphs):\n",
    "    if data.x == None:\n",
    "        print(f'Found Error in test graph {i}')\n",
    "        id_error_test.append(i)\n",
    "\n",
    "train_df = train_df.drop(id_error_train)\n",
    "test_df = test_df.drop(id_error_test)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df)\n",
    "print(test_df)\n",
    "\n",
    "for i in id_error_train:\n",
    "    train_pyg_graphs.pop(i)\n",
    "    train_netx_graphs.pop(i)\n",
    "\n",
    "for i in id_error_test:\n",
    "    test_pyg_graphs.pop(i)\n",
    "    test_netx_graphs.pop(i)\n",
    "\n",
    "print(len(train_pyg_graphs),len(train_netx_graphs), len(train_df))\n",
    "print(len(test_pyg_graphs),len(test_netx_graphs), len(test_df))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_pyg_graphs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [3], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m tqdm([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_pyg_graphs\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest_pyg_graphs\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_netx_graphs\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest_netx_graphs\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_df\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest_df\u001B[39m\u001B[38;5;124m'\u001B[39m]):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(name\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m handle:\n\u001B[1;32m----> 3\u001B[0m         pkl\u001B[38;5;241m.\u001B[39mdump(\u001B[38;5;28meval\u001B[39m(name), handle)\n",
      "File \u001B[1;32m<string>:1\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_pyg_graphs' is not defined"
     ]
    }
   ],
   "source": [
    "for name in tqdm(['train_pyg_graphs', 'test_pyg_graphs', 'train_netx_graphs', 'test_netx_graphs', 'train_df', 'test_df']):\n",
    "    with open(name+'.pkl', 'wb') as handle:\n",
    "        pkl.dump(eval(name), handle)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for name in tqdm(['train_pyg_graphs', 'test_pyg_graphs', 'train_netx_graphs', 'test_netx_graphs', 'train_df', 'test_df']):\n",
    "    with open(name+'.pkl', 'rb') as handle:\n",
    "        exec(name+' = pkl.load(handle)')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class GATClassifier(torch.nn.Module):\n",
    "    def __init__(self, embeddings_lookup, window_size=7):\n",
    "        super().__init__()\n",
    "        #torch.manual_seed(12345)\n",
    "        self.embeddings_lookup = embeddings_lookup\n",
    "        self.window_size = window_size\n",
    "        self.conv1 = GATConv(100, 10, heads=3)\n",
    "        # self.conv1 = SGConv(100, 50, K=1)\n",
    "        self.linear1 = torch.nn.Linear(10 * 3, 1)\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        pyg_graphs = []\n",
    "        if not isinstance(X, list):\n",
    "            X = [X]\n",
    "        for text in X:\n",
    "            text = text.split()\n",
    "            embeddings_for_text = []\n",
    "            G = nx.Graph()\n",
    "            for i, word in enumerate(text):\n",
    "                #embedding = fasttext_model.get_word_vector(word)\n",
    "                embedding = self.embeddings_lookup.get(word, np.zeros(100, dtype='float32'))\n",
    "                G.add_node(word, x=embedding)\n",
    "                embeddings_for_text.append(embedding)\n",
    "                for j in range(i + 1, i + self.window_size):\n",
    "                    if j < len(text):\n",
    "                        G.add_edge(word, text[j])\n",
    "            pyg_graphs.append(from_networkx(G))\n",
    "\n",
    "        predicted_probs = []\n",
    "        loader = DataLoader(pyg_graphs, batch_size=1, shuffle=False)\n",
    "        for graph in pyg_graphs:\n",
    "            graph = graph.to(device)\n",
    "            x = graph.x.to(device)\n",
    "            edge_index = graph.edge_index.to(device)\n",
    "            batch = graph.batch\n",
    "            out = self.forward(x, edge_index, batch)\n",
    "            print(out)\n",
    "            out = out.squeeze().cpu().detach().numpy()\n",
    "            predicted_probs.append([1-out, out])\n",
    "        print(predicted_probs)\n",
    "        return np.array(predicted_probs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WINDOWS DATA\\Development\\ictai journal extension\\env\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GATClassifier(\n",
      "  (conv1): GATConv(100, 10, heads=3)\n",
      "  (linear1): Linear(in_features=30, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch: 0, Epoch loss 0.3533962070941925\n",
      "Epoch: 1, Epoch loss 0.3461766541004181\n",
      "Epoch: 2, Epoch loss 0.35605940222740173\n",
      "Epoch: 3, Epoch loss 0.33927664160728455\n",
      "Epoch: 4, Epoch loss 0.33910325169563293\n",
      "Epoch: 5, Epoch loss 0.3337400555610657\n",
      "Epoch: 6, Epoch loss 0.35725685954093933\n",
      "Epoch: 7, Epoch loss 0.3405722379684448\n",
      "Epoch: 8, Epoch loss 0.34837648272514343\n",
      "Epoch: 9, Epoch loss 0.34768742322921753\n",
      "Training process has finished.\n",
      "Final loss 0.34768742322921753\n",
      "(0.7972936822863257, 0.8124896020810962, 0.7830821258855427)\n"
     ]
    }
   ],
   "source": [
    "def run_gat_classifier(train_pyg_graphs, test_pyg_graphs, train_batch_size=300, learning_rate=0.001, num_epoch=10):\n",
    "    train_loader = DataLoader(train_pyg_graphs, batch_size=train_batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_pyg_graphs, batch_size=200, shuffle=False)\n",
    "\n",
    "    gat_model = GATClassifier(embeddings_lookup=embeddings_lookup, window_size=7).to(device)\n",
    "    print(gat_model)\n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = F.binary_cross_entropy\n",
    "    optimizer = torch.optim.Adam(gat_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    gat_model.train()\n",
    "    for epoch in range(0, num_epoch):\n",
    "        for i, data in enumerate(train_loader):  # Iterate in batches over the training dataset.\n",
    "            x = data.x.to(device)\n",
    "            edge_index = data.edge_index.to(device)\n",
    "            data = data.to(device)\n",
    "            try:\n",
    "                out = gat_model(x, edge_index, data.batch)  # Perform a single forward pass.\n",
    "            except Exception as e:\n",
    "                print(data)\n",
    "                print(data.x)\n",
    "                print(data.y)\n",
    "            out = out.squeeze()\n",
    "            y = data.y.squeeze()\n",
    "            loss = loss_function(out, y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "        print(f'Epoch: {epoch}, Epoch loss {loss.item()}')\n",
    "\n",
    "    print('Training process has finished.')\n",
    "    print('Final loss', loss.item())\n",
    "\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    with torch.no_grad():\n",
    "        gat_model.eval()\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            out = gat_model(data.x, data.edge_index, data.batch)\n",
    "            pred_labels.extend(torch.round(out.squeeze()).tolist())\n",
    "            true_labels.extend(data.y.tolist())\n",
    "\n",
    "    #print('true labels ----')\n",
    "    #print(true_labels)\n",
    "    #print('pred labels ----')\n",
    "    #print(pred_labels)\n",
    "\n",
    "    results = calculate_accuracy_precision_recall(true_labels, pred_labels)\n",
    "\n",
    "    print(results)\n",
    "    return {\n",
    "        'model': gat_model,\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "gat_models_results = []\n",
    "gat_evaluation_results = []\n",
    "gat_final_models = []\n",
    "for num in global_random_number:\n",
    "    torch.manual_seed(num)\n",
    "    foo = run_gat_classifier(train_pyg_graphs, test_pyg_graphs)\n",
    "    gat_evaluation_results.append(foo['results'])\n",
    "    gat_final_models.append(foo['model'])\n",
    "    gat_models_results.append(foo)\n",
    "with open('gat_models_results.pkl', 'wb') as f:\n",
    "    pkl.dump(gat_models_results, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('gat_models_results.pkl', 'rb') as f:\n",
    "    gat_models_results = pkl.load(f)\n",
    "gat_final_models = [gat['model'] for gat in gat_models_results]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class fastTextVectorizer:\n",
    "    def __init__(self, embeddings_lookup, verbose=False, lowercase=True, minchars=3):\n",
    "        # load in pre-trained word vectors\n",
    "        print('Loading word vectors...')\n",
    "\n",
    "\n",
    "        self.idx2word = list(embeddings_lookup.keys())\n",
    "        self.word2vec = embeddings_lookup\n",
    "        self.embedding = np.array(list(embeddings_lookup.values()))\n",
    "        self.word2idx = {v:k for k,v in enumerate(self.idx2word)}\n",
    "        self.V, self.D = self.embedding.shape\n",
    "        self.verbose = verbose\n",
    "        self.lowercase = lowercase\n",
    "        self.minchars = minchars\n",
    "\n",
    "    def fit(self, data, *args):\n",
    "        pass\n",
    "\n",
    "    def transform(self, data, *args):\n",
    "        X = np.zeros((len(data), self.D))\n",
    "        n = 0\n",
    "        emptycount = 0\n",
    "        for sentence in data:\n",
    "            # Note: lower-casing the words\n",
    "            if self.lowercase:\n",
    "                tokens = sentence.lower().split()\n",
    "            else:\n",
    "                tokens = sentence.split()\n",
    "            vecs = []\n",
    "            for word in tokens:\n",
    "                if len(word) >= self.minchars and word in self.word2vec:\n",
    "                    vec = self.word2vec[word]\n",
    "                    vecs.append(vec)\n",
    "            if len(vecs) > 0:\n",
    "                vecs = np.array(vecs)\n",
    "                X[n] = vecs.mean(axis=0)\n",
    "            else:\n",
    "                emptycount += 1\n",
    "            n += 1\n",
    "        if self.verbose:\n",
    "            print(\"Number of samples with no words found / total: %s / %s\" % (emptycount, len(data)))\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, *args):\n",
    "        self.fit(X, *args)\n",
    "        return self.transform(X, *args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "def run_logistic_classifier(train_df, test_df):\n",
    "\n",
    "    vectorizer = fastTextVectorizer(embeddings_lookup)\n",
    "    train_list_corpus = train_df[\"body\"].tolist()\n",
    "\n",
    "    test_list_corpus = test_df['body'].tolist()\n",
    "    train_list_labels = train_df[\"label\"].tolist()\n",
    "    test_list_labels = test_df[\"label\"].tolist()\n",
    "\n",
    "    logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "\n",
    "    pipeline = make_pipeline(vectorizer, logreg)\n",
    "    pipeline.fit(train_list_corpus, train_list_labels)\n",
    "\n",
    "    pred_labels = pipeline.predict(test_list_corpus)\n",
    "\n",
    "    results = calculate_accuracy_precision_recall(test_list_labels, pred_labels)\n",
    "\n",
    "    print(results)\n",
    "    return {\n",
    "        'model': pipeline,\n",
    "        'results': results\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WINDOWS DATA\\Development\\ictai journal extension\\env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7891420333794047, 0.7992738743564562, 0.7830092417130696)\n"
     ]
    }
   ],
   "source": [
    "logistic_model = run_logistic_classifier(train_df, test_df)\n",
    "with open('logistic_model.pkl', 'wb') as f:\n",
    "    pkl.dump(logistic_model, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('logistic_model.pkl', 'rb') as f:\n",
    "    logistic_model = pkl.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "def visualize_graph(G, seed=10, **kwargs):\n",
    "    node_args = set(signature(nx.draw_networkx_nodes).parameters.keys())\n",
    "    node_kwargs = {k: v for k, v in kwargs.items() if k in node_args}\n",
    "    node_kwargs['node_size'] = kwargs.get('node_size') or 1300\n",
    "    node_kwargs['cmap'] = kwargs.get('cmap') or 'cool'\n",
    "\n",
    "    label_args = set(signature(nx.draw_networkx_labels).parameters.keys())\n",
    "    label_kwargs = {k: v for k, v in kwargs.items() if k in label_args}\n",
    "    label_kwargs['font_size'] = kwargs.get('font_size') or 10\n",
    "\n",
    "    pos = nx.spring_layout(G, seed=seed)\n",
    "    ax = plt.gca()\n",
    "    for source, target, data in G.edges(data=True):\n",
    "        ax.annotate(\n",
    "            '', xy=pos[target], xycoords='data', xytext=pos[source],\n",
    "            textcoords='data', arrowprops=dict(\n",
    "                arrowstyle=\"->\",\n",
    "                alpha=max(data['att'], 0.1),\n",
    "                color=data['edge_color'],\n",
    "                shrinkA=sqrt(node_kwargs['node_size']) / 2.0,\n",
    "                shrinkB=sqrt(node_kwargs['node_size']) / 2.0,\n",
    "                connectionstyle=\"arc3,rad=0.1\",\n",
    "            ))\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_color='none',\n",
    "                               **node_kwargs)\n",
    "    nx.draw_networkx_labels(G, pos,  **label_kwargs)\n",
    "\n",
    "    return ax\n",
    "\n",
    "def return_to_networkx(edge_index, edge_mask, original_graph, threshold=None, seed=10, visualize=True, **kwargs):\n",
    "\n",
    "\n",
    "    assert edge_mask.size(0) == edge_index.size(1)\n",
    "\n",
    "\n",
    "    hard_edge_mask = torch.BoolTensor([True] * edge_index.size(1),\n",
    "                                      device=edge_mask.device)\n",
    "\n",
    "    y = torch.zeros(edge_index.max().item() + 1,\n",
    "                            device=edge_index.device)\n",
    "\n",
    "    edge_color = ['black'] * edge_index.size(1)\n",
    "\n",
    "\n",
    "    if threshold is not None:\n",
    "        edge_mask = (edge_mask >= threshold).to(torch.float)\n",
    "\n",
    "\n",
    "\n",
    "    data = Data(edge_index=edge_index, att=edge_mask,\n",
    "                edge_color=edge_color, y=y, num_nodes=y.size(0)).to('cpu')\n",
    "    G = to_networkx(data, node_attrs=['y'],\n",
    "                    edge_attrs=['att', 'edge_color'])\n",
    "\n",
    "    mapping = dict(zip(range(original_graph.number_of_nodes()), original_graph.nodes()))\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    if visualize:\n",
    "        ax = visualize_graph(G, seed)\n",
    "        # plt.savefig(f'explainability\\\\{idx}\\\\original graph idx {idx}.png', dpi=500)\n",
    "        plt.show()\n",
    "\n",
    "    return G"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134509 134509 134509\n"
     ]
    }
   ],
   "source": [
    "bug_df = train_df[train_df['label']==0]\n",
    "bug_pyg_graphs = [graph for graph in train_pyg_graphs if graph.y == 0]\n",
    "bug_netx_graphs = [graph for graph in train_netx_graphs if graph.graph['y'] == 0]\n",
    "print(len(bug_df), len(bug_pyg_graphs), len(bug_netx_graphs))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bug_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [8], line 9\u001B[0m\n\u001B[0;32m      5\u001B[0m k \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m7\u001B[39m\n\u001B[0;32m      7\u001B[0m lime_explainer \u001B[38;5;241m=\u001B[39m LimeTextExplainer(class_names\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbug\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m'\u001B[39m], random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m((\u001B[38;5;28mlen\u001B[39m(bug_df))):\n\u001B[0;32m     11\u001B[0m     gat_model \u001B[38;5;241m=\u001B[39m gat_final_models[random\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m,\u001B[38;5;28mlen\u001B[39m(gat_final_models)\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     12\u001B[0m     body \u001B[38;5;241m=\u001B[39m bug_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()[idx]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'bug_df' is not defined"
     ]
    }
   ],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# idx = random.randint(0, len(bug_df)-1)\n",
    "# idx = 107258\n",
    "k = 7\n",
    "\n",
    "lime_explainer = LimeTextExplainer(class_names=['bug', 'feature'], random_state=1)\n",
    "\n",
    "for idx in range((len(bug_df))):\n",
    "\n",
    "    gat_model = gat_final_models[random.randint(0,len(gat_final_models)-1)].to(device)\n",
    "    body = bug_df['body'].tolist()[idx]\n",
    "    label = bug_df['label'].tolist()[idx]\n",
    "\n",
    "    if 20 <= len(body.split()) or len(body.split()) <= 10:\n",
    "        continue\n",
    "    instance = bug_pyg_graphs[idx]\n",
    "    if instance.x is None:\n",
    "        continue\n",
    "    print(f'TRUE LABEL IS '+('BUG' if label==0 else 'FEATURE'))\n",
    "    exp = lime_explainer.explain_instance(body, gat_model.predict_proba, num_features=20)\n",
    "\n",
    "    topk_words = exp.as_list()#/label=test_df['label'].tolist()[idx])\n",
    "    print(f'The original words with their importance: {topk_words}')\n",
    "    if label==0:\n",
    "        topk_words = [word for word, pred in topk_words if pred<0]\n",
    "    else:\n",
    "        topk_words = [word for word, pred in topk_words if pred>=0]\n",
    "\n",
    "    if len(topk_words)>=k:\n",
    "        topk_words = topk_words[:k]\n",
    "    # print(f'The top-{k} words from LIME are: {topk_words}')\n",
    "\n",
    "\n",
    "\n",
    "    explainer = GNNExplainer(gat_model.to('cpu'))\n",
    "    x = instance.x.to('cpu')\n",
    "    edge_index = instance.edge_index.to('cpu')\n",
    "    node_feat_mask, edge_mask = explainer.explain_graph(x, edge_index)\n",
    "\n",
    "    # exp.show_in_notebook(text=True)\n",
    "\n",
    "    import os\n",
    "    # os.makedirs(f'explainability\\\\{idx}')\n",
    "\n",
    "\n",
    "    G = return_to_networkx(edge_index, edge_mask, bug_netx_graphs[idx], threshold=0.5)\n",
    "    words = list(G.nodes())\n",
    "    # print(words)\n",
    "\n",
    "    for word in words:\n",
    "        if word not in topk_words:\n",
    "            G.remove_node(word)\n",
    "    ax = visualize_graph(G, 10)\n",
    "    meaningful_edges = [edge for edge in list(G.edges.data(\"att\")) if edge[2]==1.0]\n",
    "    if len(meaningful_edges)>=3:\n",
    "        print(idx)\n",
    "\n",
    "    # plt.savefig(f'explainability\\\\{idx}\\\\graph with lime idx {idx}.png', dpi=500)\n",
    "    # plt.show()\n",
    "    print(idx)\n",
    "    # exp.save_to_file(f'explainability\\\\{idx}/lime explainer idx {idx}.html')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}