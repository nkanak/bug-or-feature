Train size 273225
Test size 134574
Dim: 32
finished...
GATClassifier(
  (conv1): GATConv(32, 10, heads=5)
  (linear1): Linear(in_features=50, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
Epoch: 0, Epoch loss 0.34298795461654663
Epoch: 1, Epoch loss 0.3374790847301483
Epoch: 2, Epoch loss 0.33562299609184265
Epoch: 3, Epoch loss 0.3311786949634552
Epoch: 4, Epoch loss 0.33949926495552063
Epoch: 5, Epoch loss 0.31804460287094116
Epoch: 6, Epoch loss 0.32048267126083374
Epoch: 7, Epoch loss 0.3357332646846771
Epoch: 8, Epoch loss 0.33240634202957153
Epoch: 9, Epoch loss 0.3299121856689453
Training process has finished.
Final loss 0.3299121856689453
(0.8022129088828451, 0.8139437085098075, 0.7920436110664543)
Dim: 64
finished...
GATClassifier(
  (conv1): GATConv(64, 10, heads=5)
  (linear1): Linear(in_features=50, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
Epoch: 0, Epoch loss 0.3394955098628998
Epoch: 1, Epoch loss 0.33992999792099
Epoch: 2, Epoch loss 0.34134459495544434
Epoch: 3, Epoch loss 0.33637478947639465
Epoch: 4, Epoch loss 0.3357684314250946
Epoch: 5, Epoch loss 0.3236657977104187
Epoch: 6, Epoch loss 0.3183627426624298
Epoch: 7, Epoch loss 0.32871317863464355
Epoch: 8, Epoch loss 0.3214994966983795
Epoch: 9, Epoch loss 0.328524112701416
Training process has finished.
Final loss 0.328524112701416
(0.8033275372657422, 0.8157340236597334, 0.7921166859097087)
Dim: 100
finished...
GATClassifier(
  (conv1): GATConv(100, 10, heads=5)
  (linear1): Linear(in_features=50, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
Epoch: 0, Epoch loss 0.35238116979599
Epoch: 1, Epoch loss 0.34674713015556335
Epoch: 2, Epoch loss 0.3391154706478119
Epoch: 3, Epoch loss 0.34003520011901855
Epoch: 4, Epoch loss 0.33348435163497925
Epoch: 5, Epoch loss 0.3313950002193451
Epoch: 6, Epoch loss 0.3286541998386383
Epoch: 7, Epoch loss 0.3315744400024414
Epoch: 8, Epoch loss 0.33053940534591675
Epoch: 9, Epoch loss 0.3187367916107178
Training process has finished.
Final loss 0.3187367916107178
(0.8031937818597946, 0.8111866318413867, 0.7988688014264209)
Dim: 128
finished...
GATClassifier(
  (conv1): GATConv(128, 10, heads=5)
  (linear1): Linear(in_features=50, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
Epoch: 0, Epoch loss 0.33918848633766174
Epoch: 1, Epoch loss 0.33015546202659607
Epoch: 2, Epoch loss 0.33907803893089294
Epoch: 3, Epoch loss 0.33975252509117126
Epoch: 4, Epoch loss 0.32069477438926697
Epoch: 5, Epoch loss 0.31273341178894043
Epoch: 6, Epoch loss 0.31927138566970825
Epoch: 7, Epoch loss 0.30977699160575867
Epoch: 8, Epoch loss 0.3073878586292267
Epoch: 9, Epoch loss 0.3017420172691345
Training process has finished.
Final loss 0.3017420172691345
(0.8032755212745404, 0.8159619179609238, 0.7916343919442292)
Dim: 150
finished...
GATClassifier(
  (conv1): GATConv(150, 10, heads=5)
  (linear1): Linear(in_features=50, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
Epoch: 0, Epoch loss 0.35570651292800903
Epoch: 1, Epoch loss 0.34705981612205505
Epoch: 2, Epoch loss 0.32831957936286926
Epoch: 3, Epoch loss 0.33602800965309143
Epoch: 4, Epoch loss 0.3353780210018158
Epoch: 5, Epoch loss 0.3173713684082031
Epoch: 6, Epoch loss 0.3177655041217804
Epoch: 7, Epoch loss 0.33618101477622986
Epoch: 8, Epoch loss 0.33222466707229614
Epoch: 9, Epoch loss 0.318366676568985
Training process has finished.
Final loss 0.318366676568985
(0.803037733886189, 0.8116830004610145, 0.7976849889656986)
[[32, 0.8022129088828451], [64, 0.8033275372657422], [100, 0.8031937818597946], [128, 0.8032755212745404], [150, 0.803037733886189]]
finished...
Epochs: 5
finished...
GATClassifier(
  (conv1): GATConv(100, 10, heads=5)
  (linear1): Linear(in_features=50, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
Epoch: 0, Epoch loss 0.3556608557701111
Epoch: 1, Epoch loss 0.33977046608924866
Epoch: 2, Epoch loss 0.3406955599784851
Epoch: 3, Epoch loss 0.3303530514240265
Epoch: 4, Epoch loss 0.34115472435951233
Training process has finished.
Final loss 0.34115472435951233
(0.8026959145154339, 0.8115540872360373, 0.7970127004077576)
Epochs: 10
finished...
GATClassifier(
  (conv1): GATConv(100, 10, heads=5)
  (linear1): Linear(in_features=50, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
Epoch: 0, Epoch loss 0.34311774373054504
Epoch: 1, Epoch loss 0.35022473335266113
Epoch: 2, Epoch loss 0.3486998379230499
Epoch: 3, Epoch loss 0.31921863555908203
Epoch: 4, Epoch loss 0.331918329000473
Epoch: 5, Epoch loss 0.33568552136421204
Epoch: 6, Epoch loss 0.32617196440696716
Epoch: 7, Epoch loss 0.33946895599365234
Epoch: 8, Epoch loss 0.31848272681236267
Epoch: 9, Epoch loss 0.32689911127090454
Training process has finished.
Final loss 0.32689911127090454
(0.8021757546034152, 0.8106910853116499, 0.7970419303450594)
Epochs: 20
finished...
GATClassifier(
  (conv1): GATConv(100, 10, heads=5)
  (linear1): Linear(in_features=50, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
Epoch: 0, Epoch loss 0.336954265832901
Epoch: 1, Epoch loss 0.33745118975639343
Epoch: 2, Epoch loss 0.3387914001941681
Epoch: 3, Epoch loss 0.3378683924674988
Epoch: 4, Epoch loss 0.32872897386550903
Epoch: 5, Epoch loss 0.33769649267196655
Epoch: 6, Epoch loss 0.3319990932941437
Epoch: 7, Epoch loss 0.3157351016998291
Epoch: 8, Epoch loss 0.3298049569129944
Epoch: 9, Epoch loss 0.30358830094337463
Epoch: 10, Epoch loss 0.314604789018631
Epoch: 11, Epoch loss 0.32332292199134827
Epoch: 12, Epoch loss 0.31188541650772095
Epoch: 13, Epoch loss 0.31682246923446655
Epoch: 14, Epoch loss 0.3057272434234619
Epoch: 15, Epoch loss 0.30928972363471985
Epoch: 16, Epoch loss 0.3090232312679291
Epoch: 17, Epoch loss 0.3087750971317291
Epoch: 18, Epoch loss 0.3169035315513611
Epoch: 19, Epoch loss 0.30358362197875977
Training process has finished.
Final loss 0.30358362197875977
(0.801469823294247, 0.808239224265358, 0.799131870862137)
Epochs: 30
finished...
GATClassifier(
  (conv1): GATConv(100, 10, heads=5)
  (linear1): Linear(in_features=50, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
Epoch: 0, Epoch loss 0.3432689607143402
Epoch: 1, Epoch loss 0.33911678194999695
Epoch: 2, Epoch loss 0.3337779939174652
Epoch: 3, Epoch loss 0.3229852616786957
Epoch: 4, Epoch loss 0.3326408267021179
Epoch: 5, Epoch loss 0.3168148100376129
Epoch: 6, Epoch loss 0.31880730390548706
Epoch: 7, Epoch loss 0.3078572452068329
Epoch: 8, Epoch loss 0.31868287920951843
Epoch: 9, Epoch loss 0.3164811134338379
Epoch: 10, Epoch loss 0.31921735405921936
Epoch: 11, Epoch loss 0.3133770525455475
Epoch: 12, Epoch loss 0.3081488311290741
Epoch: 13, Epoch loss 0.3110125660896301
Epoch: 14, Epoch loss 0.3168681561946869
Epoch: 15, Epoch loss 0.30766016244888306
Epoch: 16, Epoch loss 0.293075293302536
Epoch: 17, Epoch loss 0.31938162446022034
Epoch: 18, Epoch loss 0.2942333221435547
Epoch: 19, Epoch loss 0.3075468838214874
Epoch: 20, Epoch loss 0.31481578946113586
Epoch: 21, Epoch loss 0.31268826127052307
Epoch: 22, Epoch loss 0.2968965470790863
Epoch: 23, Epoch loss 0.3080956041812897
Epoch: 24, Epoch loss 0.29427266120910645
Epoch: 25, Epoch loss 0.2864360213279724
Epoch: 26, Epoch loss 0.27976539731025696
Epoch: 27, Epoch loss 0.2962642014026642
Epoch: 28, Epoch loss 0.2913133203983307
Epoch: 29, Epoch loss 0.3123587965965271
Training process has finished.
Final loss 0.3123587965965271
(0.8016035787001947, 0.8064426099474132, 0.802376393902635)
Epochs: 50
finished...
GATClassifier(
  (conv1): GATConv(100, 10, heads=5)
  (linear1): Linear(in_features=50, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
Epoch: 0, Epoch loss 0.34418758749961853
Epoch: 1, Epoch loss 0.3547551929950714
Epoch: 2, Epoch loss 0.3371813893318176
Epoch: 3, Epoch loss 0.3362410068511963
Epoch: 4, Epoch loss 0.32662397623062134
Epoch: 5, Epoch loss 0.3252280056476593
Epoch: 6, Epoch loss 0.3122935891151428
Epoch: 7, Epoch loss 0.3232310712337494
Epoch: 8, Epoch loss 0.33233585953712463
Epoch: 9, Epoch loss 0.3359605669975281
Epoch: 10, Epoch loss 0.32286760210990906
Epoch: 11, Epoch loss 0.32320183515548706
Epoch: 12, Epoch loss 0.31362035870552063
Epoch: 13, Epoch loss 0.28874510526657104
Epoch: 14, Epoch loss 0.294283926486969
Epoch: 15, Epoch loss 0.3123096823692322
Epoch: 16, Epoch loss 0.2948908805847168
Epoch: 17, Epoch loss 0.30985477566719055
Epoch: 18, Epoch loss 0.31647777557373047
Epoch: 19, Epoch loss 0.31197553873062134
Epoch: 20, Epoch loss 0.2846445143222809
Epoch: 21, Epoch loss 0.3030236065387726
Epoch: 22, Epoch loss 0.3102264106273651
Epoch: 23, Epoch loss 0.33207058906555176
Epoch: 24, Epoch loss 0.30079323053359985
Epoch: 25, Epoch loss 0.30546852946281433
Epoch: 26, Epoch loss 0.31739139556884766
Epoch: 27, Epoch loss 0.31322580575942993
Epoch: 28, Epoch loss 0.3110457956790924
Epoch: 29, Epoch loss 0.2953765392303467
Epoch: 30, Epoch loss 0.30353617668151855
Epoch: 31, Epoch loss 0.30122822523117065
Epoch: 32, Epoch loss 0.30624622106552124
Epoch: 33, Epoch loss 0.286559134721756
Epoch: 34, Epoch loss 0.29351806640625
Epoch: 35, Epoch loss 0.2736984193325043
Epoch: 36, Epoch loss 0.2963593900203705
Epoch: 37, Epoch loss 0.30174264311790466
Epoch: 38, Epoch loss 0.3017653524875641
Epoch: 39, Epoch loss 0.2875155508518219
Epoch: 40, Epoch loss 0.28039029240608215
Epoch: 41, Epoch loss 0.29937171936035156
Epoch: 42, Epoch loss 0.2887853980064392
Epoch: 43, Epoch loss 0.2796303331851959
Epoch: 44, Epoch loss 0.302481085062027
Epoch: 45, Epoch loss 0.30129677057266235
Epoch: 46, Epoch loss 0.29546934366226196
Epoch: 47, Epoch loss 0.2703814208507538
Epoch: 48, Epoch loss 0.2941252589225769
Epoch: 49, Epoch loss 0.3010973334312439
Training process has finished.
Final loss 0.3010973334312439
(0.800719306849763, 0.8082628217476994, 0.7971588500942666)
Epochs: 100
finished...
GATClassifier(
  (conv1): GATConv(100, 10, heads=5)
  (linear1): Linear(in_features=50, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
Epoch: 0, Epoch loss 0.34559234976768494
Epoch: 1, Epoch loss 0.33963850140571594
Epoch: 2, Epoch loss 0.34454187750816345
Epoch: 3, Epoch loss 0.3405877649784088
Epoch: 4, Epoch loss 0.3296930491924286
Epoch: 5, Epoch loss 0.3153465688228607
Epoch: 6, Epoch loss 0.32596713304519653
Epoch: 7, Epoch loss 0.3224027156829834
Epoch: 8, Epoch loss 0.3159434497356415
Epoch: 9, Epoch loss 0.3194856345653534
Epoch: 10, Epoch loss 0.30362558364868164
Epoch: 11, Epoch loss 0.3035445511341095
Epoch: 12, Epoch loss 0.30954188108444214
Epoch: 13, Epoch loss 0.30249953269958496
Epoch: 14, Epoch loss 0.30478358268737793
Epoch: 15, Epoch loss 0.3182961940765381
Epoch: 16, Epoch loss 0.3003661632537842
Epoch: 17, Epoch loss 0.2983185946941376
Epoch: 18, Epoch loss 0.2983461320400238
Epoch: 19, Epoch loss 0.2962000072002411
Epoch: 20, Epoch loss 0.29835739731788635
Epoch: 21, Epoch loss 0.2960328161716461
Epoch: 22, Epoch loss 0.3133275806903839
Epoch: 23, Epoch loss 0.28932198882102966
Epoch: 24, Epoch loss 0.31049981713294983
Epoch: 25, Epoch loss 0.29741600155830383
Epoch: 26, Epoch loss 0.28098735213279724
Epoch: 27, Epoch loss 0.2948470711708069
Epoch: 28, Epoch loss 0.2929061949253082
Epoch: 29, Epoch loss 0.30465221405029297
Epoch: 30, Epoch loss 0.3160211741924286
Epoch: 31, Epoch loss 0.29652395844459534
Epoch: 32, Epoch loss 0.28368157148361206
Epoch: 33, Epoch loss 0.27625182271003723
Epoch: 34, Epoch loss 0.30847856402397156
Epoch: 35, Epoch loss 0.29643484950065613
Epoch: 36, Epoch loss 0.29752400517463684
Epoch: 37, Epoch loss 0.3137683570384979
Epoch: 38, Epoch loss 0.2769472002983093
Epoch: 39, Epoch loss 0.2939140796661377
Epoch: 40, Epoch loss 0.29788798093795776
Epoch: 41, Epoch loss 0.2816210389137268
Epoch: 42, Epoch loss 0.300936758518219
Epoch: 43, Epoch loss 0.2825939357280731
Epoch: 44, Epoch loss 0.27959588170051575
Epoch: 45, Epoch loss 0.2996428310871124
Epoch: 46, Epoch loss 0.30178460478782654
Epoch: 47, Epoch loss 0.2920507788658142
Epoch: 48, Epoch loss 0.3040620982646942
Epoch: 49, Epoch loss 0.2850314676761627
Epoch: 50, Epoch loss 0.2881772220134735
Epoch: 51, Epoch loss 0.2958342432975769
Epoch: 52, Epoch loss 0.28142404556274414
Epoch: 53, Epoch loss 0.2871219217777252
Epoch: 54, Epoch loss 0.30588558316230774
Epoch: 55, Epoch loss 0.2961917221546173
Epoch: 56, Epoch loss 0.28730326890945435
Epoch: 57, Epoch loss 0.2842377722263336
Epoch: 58, Epoch loss 0.28635677695274353
Epoch: 59, Epoch loss 0.2856012284755707
Epoch: 60, Epoch loss 0.2739573121070862
Epoch: 61, Epoch loss 0.28486213088035583
Epoch: 62, Epoch loss 0.27594834566116333
Epoch: 63, Epoch loss 0.2896648049354553
Epoch: 64, Epoch loss 0.2879529595375061
Epoch: 65, Epoch loss 0.30529484152793884
Epoch: 66, Epoch loss 0.2830008566379547
Epoch: 67, Epoch loss 0.31843656301498413
Epoch: 68, Epoch loss 0.27868443727493286
Epoch: 69, Epoch loss 0.2927188575267792
Epoch: 70, Epoch loss 0.2888929843902588
Epoch: 71, Epoch loss 0.2849154770374298
Epoch: 72, Epoch loss 0.2525673806667328
Epoch: 73, Epoch loss 0.3070869445800781
Epoch: 74, Epoch loss 0.29300937056541443
Epoch: 75, Epoch loss 0.27783656120300293
Epoch: 76, Epoch loss 0.2876260280609131
Epoch: 77, Epoch loss 0.27427178621292114
Epoch: 78, Epoch loss 0.27982190251350403
Epoch: 79, Epoch loss 0.2794131338596344
Epoch: 80, Epoch loss 0.332439124584198
Epoch: 81, Epoch loss 0.28897303342819214
Epoch: 82, Epoch loss 0.290104478597641
Epoch: 83, Epoch loss 0.28229257464408875
Epoch: 84, Epoch loss 0.28911811113357544
Epoch: 85, Epoch loss 0.30271807312965393
Epoch: 86, Epoch loss 0.26985350251197815
Epoch: 87, Epoch loss 0.2653264105319977
Epoch: 88, Epoch loss 0.2823360860347748
Epoch: 89, Epoch loss 0.27692002058029175
Epoch: 90, Epoch loss 0.30694350600242615
Epoch: 91, Epoch loss 0.28188443183898926
Epoch: 92, Epoch loss 0.31048712134361267
Epoch: 93, Epoch loss 0.27437564730644226
Epoch: 94, Epoch loss 0.2812502980232239
Epoch: 95, Epoch loss 0.2962101101875305
Epoch: 96, Epoch loss 0.29293185472488403
Epoch: 97, Epoch loss 0.28083160519599915
Epoch: 98, Epoch loss 0.308814138174057
Epoch: 99, Epoch loss 0.30812209844589233
Training process has finished.
Final loss 0.30812209844589233
(0.8001471309465424, 0.8041096692931837, 0.8024056238399369)
Epochs: 200
finished...
GATClassifier(
  (conv1): GATConv(100, 10, heads=5)
  (linear1): Linear(in_features=50, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
Epoch: 0, Epoch loss 0.33999723196029663
Epoch: 1, Epoch loss 0.3396153151988983
Epoch: 2, Epoch loss 0.3260996639728546
Epoch: 3, Epoch loss 0.3426288068294525
Epoch: 4, Epoch loss 0.3394174575805664
Epoch: 5, Epoch loss 0.3391166627407074
Epoch: 6, Epoch loss 0.3221468925476074
Epoch: 7, Epoch loss 0.32978540658950806
Epoch: 8, Epoch loss 0.31687214970588684
Epoch: 9, Epoch loss 0.3029467761516571
Epoch: 10, Epoch loss 0.3193665146827698
Epoch: 11, Epoch loss 0.32343724370002747
Epoch: 12, Epoch loss 0.31387659907341003
Epoch: 13, Epoch loss 0.3128686845302582
Epoch: 14, Epoch loss 0.30482858419418335
Epoch: 15, Epoch loss 0.32041865587234497
Epoch: 16, Epoch loss 0.29971837997436523
Epoch: 17, Epoch loss 0.3191179931163788
Epoch: 18, Epoch loss 0.30695489048957825
Epoch: 19, Epoch loss 0.3295101225376129
Epoch: 20, Epoch loss 0.2942712604999542
Epoch: 21, Epoch loss 0.2949066162109375
Epoch: 22, Epoch loss 0.3182278275489807
Epoch: 23, Epoch loss 0.2762041389942169
Epoch: 24, Epoch loss 0.3059637248516083
Epoch: 25, Epoch loss 0.2903616726398468
Epoch: 26, Epoch loss 0.29201942682266235
Epoch: 27, Epoch loss 0.2856275737285614
Epoch: 28, Epoch loss 0.29555320739746094
Epoch: 29, Epoch loss 0.30486610531806946
Epoch: 30, Epoch loss 0.2697531282901764
Epoch: 31, Epoch loss 0.2911406457424164
Epoch: 32, Epoch loss 0.30530497431755066
Epoch: 33, Epoch loss 0.2953071594238281
Epoch: 34, Epoch loss 0.29075855016708374
Epoch: 35, Epoch loss 0.2901623547077179
Epoch: 36, Epoch loss 0.2998560070991516
Epoch: 37, Epoch loss 0.29710930585861206
Epoch: 38, Epoch loss 0.2903502285480499
Epoch: 39, Epoch loss 0.29615309834480286
Epoch: 40, Epoch loss 0.2747068703174591
Epoch: 41, Epoch loss 0.29101911187171936
Epoch: 42, Epoch loss 0.3007177710533142
Epoch: 43, Epoch loss 0.2992548644542694
Epoch: 44, Epoch loss 0.3030422329902649
Epoch: 45, Epoch loss 0.29178890585899353
Epoch: 46, Epoch loss 0.2966116964817047
Epoch: 47, Epoch loss 0.289125919342041
Epoch: 48, Epoch loss 0.27544453740119934
Epoch: 49, Epoch loss 0.283343642950058
Epoch: 50, Epoch loss 0.2851256728172302
Epoch: 51, Epoch loss 0.29117605090141296
Epoch: 52, Epoch loss 0.2720641493797302
Epoch: 53, Epoch loss 0.28626149892807007
Epoch: 54, Epoch loss 0.2814426124095917
Epoch: 55, Epoch loss 0.2646867632865906
Epoch: 56, Epoch loss 0.29680612683296204
Epoch: 57, Epoch loss 0.2753606140613556
Epoch: 58, Epoch loss 0.27356505393981934
Epoch: 59, Epoch loss 0.2924145758152008
Epoch: 60, Epoch loss 0.2717605233192444
Epoch: 61, Epoch loss 0.29492563009262085
Epoch: 62, Epoch loss 0.3016664981842041
Epoch: 63, Epoch loss 0.2701922655105591
Epoch: 64, Epoch loss 0.26914557814598083
Epoch: 65, Epoch loss 0.283535897731781
Epoch: 66, Epoch loss 0.2654777765274048
Epoch: 67, Epoch loss 0.26606863737106323
Epoch: 68, Epoch loss 0.288617879152298
Epoch: 69, Epoch loss 0.28382694721221924
Epoch: 70, Epoch loss 0.28917115926742554
Epoch: 71, Epoch loss 0.28633978962898254
Epoch: 72, Epoch loss 0.2952653765678406
Epoch: 73, Epoch loss 0.2663504183292389
Epoch: 74, Epoch loss 0.2812094986438751
Epoch: 75, Epoch loss 0.2943566143512726
Epoch: 76, Epoch loss 0.27092793583869934
Epoch: 77, Epoch loss 0.2766153812408447
Epoch: 78, Epoch loss 0.28568369150161743
Epoch: 79, Epoch loss 0.2781125009059906
Epoch: 80, Epoch loss 0.2686581611633301
Epoch: 81, Epoch loss 0.2917165160179138
Epoch: 82, Epoch loss 0.28036248683929443
Epoch: 83, Epoch loss 0.2802477180957794
Epoch: 84, Epoch loss 0.2867586016654968
Epoch: 85, Epoch loss 0.28217434883117676
Epoch: 86, Epoch loss 0.26383161544799805
Epoch: 87, Epoch loss 0.28178921341896057
Epoch: 88, Epoch loss 0.29732710123062134
Epoch: 89, Epoch loss 0.28124335408210754
Epoch: 90, Epoch loss 0.30157071352005005
Epoch: 91, Epoch loss 0.2840925455093384
Epoch: 92, Epoch loss 0.30093225836753845
Epoch: 93, Epoch loss 0.25949960947036743
Epoch: 94, Epoch loss 0.2723469138145447
Epoch: 95, Epoch loss 0.2816322147846222
Epoch: 96, Epoch loss 0.2988439202308655
Epoch: 97, Epoch loss 0.2795482575893402
Epoch: 98, Epoch loss 0.282867431640625
Epoch: 99, Epoch loss 0.29732364416122437
Epoch: 100, Epoch loss 0.291481077671051
Epoch: 101, Epoch loss 0.29491177201271057
Epoch: 102, Epoch loss 0.26839154958724976
Epoch: 103, Epoch loss 0.2711208462715149
Epoch: 104, Epoch loss 0.2907077372074127
Epoch: 105, Epoch loss 0.28947219252586365
Epoch: 106, Epoch loss 0.2868306338787079
Epoch: 107, Epoch loss 0.2894588112831116
Epoch: 108, Epoch loss 0.2825758755207062
Epoch: 109, Epoch loss 0.2758766710758209
Epoch: 110, Epoch loss 0.26374882459640503
Epoch: 111, Epoch loss 0.29248279333114624
Epoch: 112, Epoch loss 0.30628523230552673
Epoch: 113, Epoch loss 0.29959097504615784
Epoch: 114, Epoch loss 0.27801182866096497
Epoch: 115, Epoch loss 0.28542813658714294
Epoch: 116, Epoch loss 0.3077561557292938
Epoch: 117, Epoch loss 0.2813398540019989
Epoch: 118, Epoch loss 0.24261996150016785
Epoch: 119, Epoch loss 0.31116825342178345
Epoch: 120, Epoch loss 0.31398889422416687
Epoch: 121, Epoch loss 0.29646340012550354
Epoch: 122, Epoch loss 0.28377702832221985
Epoch: 123, Epoch loss 0.2716931700706482
Epoch: 124, Epoch loss 0.2954404354095459
Epoch: 125, Epoch loss 0.27488940954208374
Epoch: 126, Epoch loss 0.29636070132255554
Epoch: 127, Epoch loss 0.28074219822883606
Epoch: 128, Epoch loss 0.28738030791282654
Epoch: 129, Epoch loss 0.27024978399276733
Epoch: 130, Epoch loss 0.32817402482032776
Epoch: 131, Epoch loss 0.30458059906959534
Epoch: 132, Epoch loss 0.2694198191165924
Epoch: 133, Epoch loss 0.2876627445220947
Epoch: 134, Epoch loss 0.2996796667575836
Epoch: 135, Epoch loss 0.2831438183784485
Epoch: 136, Epoch loss 0.2978191077709198
Epoch: 137, Epoch loss 0.2926909327507019
Epoch: 138, Epoch loss 0.30734983086586
Epoch: 139, Epoch loss 0.28203508257865906
Epoch: 140, Epoch loss 0.27860310673713684
Epoch: 141, Epoch loss 0.2649534344673157
Epoch: 142, Epoch loss 0.28323355317115784
Epoch: 143, Epoch loss 0.2943984568119049
Epoch: 144, Epoch loss 0.28400954604148865
Epoch: 145, Epoch loss 0.30727311968803406
Epoch: 146, Epoch loss 0.29204025864601135
Epoch: 147, Epoch loss 0.25472643971443176
Epoch: 148, Epoch loss 0.26388153433799744
Epoch: 149, Epoch loss 0.27435463666915894
Epoch: 150, Epoch loss 0.2854413390159607
Epoch: 151, Epoch loss 0.2748190462589264
Epoch: 152, Epoch loss 0.26256731152534485
Epoch: 153, Epoch loss 0.2918841540813446
Epoch: 154, Epoch loss 0.26910749077796936
Epoch: 155, Epoch loss 0.2791011929512024
Epoch: 156, Epoch loss 0.26480063796043396
Epoch: 157, Epoch loss 0.290282666683197
Epoch: 158, Epoch loss 0.2868022322654724
Epoch: 159, Epoch loss 0.25620606541633606
Epoch: 160, Epoch loss 0.283351868391037
Epoch: 161, Epoch loss 0.29548439383506775
Epoch: 162, Epoch loss 0.2550860345363617
Epoch: 163, Epoch loss 0.27383318543434143
Epoch: 164, Epoch loss 0.2694545388221741
Epoch: 165, Epoch loss 0.32558444142341614
Epoch: 166, Epoch loss 0.3028026521205902
Epoch: 167, Epoch loss 0.26243022084236145
Epoch: 168, Epoch loss 0.2966578006744385
Epoch: 169, Epoch loss 0.2698923647403717
Epoch: 170, Epoch loss 0.27136096358299255
Epoch: 171, Epoch loss 0.275063157081604
Epoch: 172, Epoch loss 0.28898200392723083
Epoch: 173, Epoch loss 0.2826842665672302
Epoch: 174, Epoch loss 0.27691617608070374
Epoch: 175, Epoch loss 0.29069164395332336
Epoch: 176, Epoch loss 0.2887601852416992
Epoch: 177, Epoch loss 0.2927789092063904
Epoch: 178, Epoch loss 0.2856191098690033
Epoch: 179, Epoch loss 0.2744276821613312
Epoch: 180, Epoch loss 0.26557061076164246
Epoch: 181, Epoch loss 0.28269168734550476
Epoch: 182, Epoch loss 0.2785981595516205
Epoch: 183, Epoch loss 0.2777574360370636
Epoch: 184, Epoch loss 0.2986665666103363
Epoch: 185, Epoch loss 0.2770942151546478
Epoch: 186, Epoch loss 0.2847957909107208
Epoch: 187, Epoch loss 0.29216787219047546
Epoch: 188, Epoch loss 0.28319722414016724
Epoch: 189, Epoch loss 0.2602224051952362
Epoch: 190, Epoch loss 0.2930312156677246
Epoch: 191, Epoch loss 0.27567028999328613
Epoch: 192, Epoch loss 0.2984355390071869
Epoch: 193, Epoch loss 0.2681099474430084
Epoch: 194, Epoch loss 0.28687211871147156
Epoch: 195, Epoch loss 0.2940954267978668
Epoch: 196, Epoch loss 0.2532714009284973
Epoch: 197, Epoch loss 0.2748400568962097
Epoch: 198, Epoch loss 0.2472475916147232
Epoch: 199, Epoch loss 0.2878517210483551
Training process has finished.
Final loss 0.2878517210483551
(0.7973531291334136, 0.8174035109369697, 0.7744325738421292)
[[5, 0.8026959145154339], [10, 0.8021757546034152], [20, 0.801469823294247], [30, 0.8016035787001947], [50, 0.800719306849763], [100, 0.8001471309465424], [200, 0.7973531291334136]]
Attention heads: 1
finished...
GATClassifier(
  (conv1): GATConv(100, 10, heads=1)
  (linear1): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
Epoch: 0, Epoch loss 0.34750494360923767
Epoch: 1, Epoch loss 0.34763750433921814
Epoch: 2, Epoch loss 0.3513529598712921
Epoch: 3, Epoch loss 0.35352444648742676
Epoch: 4, Epoch loss 0.3519526422023773
Epoch: 5, Epoch loss 0.35652390122413635
Epoch: 6, Epoch loss 0.3551201820373535
Epoch: 7, Epoch loss 0.3535136282444
Epoch: 8, Epoch loss 0.3553072512149811
Epoch: 9, Epoch loss 0.35988476872444153
Training process has finished.
Final loss 0.35988476872444153
(0.8017447649620283, 0.8147137321129692, 0.7896613711763588)
Attention heads: 2
finished...
GATClassifier(
  (conv1): GATConv(100, 10, heads=2)
  (linear1): Linear(in_features=20, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
Epoch: 0, Epoch loss 0.34630972146987915
